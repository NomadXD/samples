# Copyright OpenSearch Contributors
# SPDX-License-Identifier: Apache-2.0

# Default values for data-prepper.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 2

global:
  # Set if you want to change the default docker registry, e.g. a private one.
  dockerRegistry: ""

image:
  # -- The image repository from which to pull the Data Prepper image
  repository: opensearchproject/data-prepper
  # -- The image tag to pull. Default: IfNotPresent
  pullPolicy: IfNotPresent
  # -- Overrides the image tag whose default is the chart appVersion.
  tag: ""

# -- List of imagePullSecrets to use if the Docker image is stored in a private registry
imagePullSecrets: []
# -- Override the default name for the deployment
nameOverride: ""
# -- Override the default fullname for the deployment
fullnameOverride: ""

# -- Extra environment variables to pass to the Data Prepper container
extraEnvs: []
  # - name: "JAVA_OPTS"
  #   value: "-Dlog4j2.debug=true"

# Check https://opensearch.org/docs/latest/data-prepper/managing-data-prepper/configuring-data-prepper/
# for more information on the configuration options
# -- Data Prepper configuration
config:
  # -- Main Data Prepper configuration file content
  data-prepper-config.yaml: |
    ssl: false
    peer_forwarder:
      discovery_mode: "dns"
      domain_name: "data-prepper-headless.opensearch-stack.svc.cluster.local"
    # circuit_breakers:
    #   heap:
    #     usage: 2gb
    #     reset: 30s
    #     check_interval: 5s

  # -- Log4j2 configuration for Data Prepper logging
  log4j2-rolling.properties: |
    #
    # Copyright OpenSearch Contributors
    # SPDX-License-Identifier: Apache-2.0
    #

    status = error
    dest = err
    name = PropertiesConfig

    property.filename = log/data-prepper/data-prepper.log

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{ISO8601} [%t] %-5p %40C - %m%n

    appender.rolling.type = RollingFile
    appender.rolling.name = RollingFile
    appender.rolling.fileName = ${filename}
    appender.rolling.filePattern = logs/data-prepper.log.%d{MM-dd-yy-HH}-%i.gz
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{ISO8601} [%t] %-5p %40C - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
    appender.rolling.policies.time.interval = 1
    appender.rolling.policies.time.modulate = true
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 168

    rootLogger.level = debug
    rootLogger.appenderRef.stdout.ref = STDOUT
    rootLogger.appenderRef.file.ref = RollingFile

    logger.pipeline.name = org.opensearch.dataprepper.pipeline
    logger.pipeline.level = info

    logger.parser.name = org.opensearch.dataprepper.parser
    logger.parser.level = info

    logger.plugins.name = org.opensearch.dataprepper.plugins
    logger.plugins.level = info

# For OpenSearch Data Prepper is crucial for defining the behavior and structure of your data processing pipelines.
# Each pipeline is defined with a unique name and can include `source`, `processor`, and `sink` components to ingest,
# process, and output data respectively. This flexible configuration allows for the creation of complex data processing
# flows, including the routing of data between pipelines.
# For detailed information on the available options and to get the most up-to-date guidance on configuring `pipeline.yaml`,
# please consult the [OpenSearch Documentation on Pipelines](https://opensearch.org/docs/2.4/data-prepper/pipelines/pipelines/).
# This resource provides comprehensive examples and explanations of each component, ensuring you can tailor your Data Prepper
# deployment to meet your specific data processing needs.

# -- Pipeline configuration
pipelineConfig:
  # If 'true', a secret containing a demo pipeline configuration with random source and stdout sink will be created.
  # If left undefined, the demo pipeline will be used only when no other pipeline is configured below
  demoPipeline: ""
  # -- The name of the existing secret containing the pipeline configuration.
  # If enabled is false existingSecret is used. The existingSecret must have a key named `pipelines.yaml`.
  existingSecret: ""
  # If enabled, a secret containing the pipeline configuration will be created based on the 'config' section below.
  enabled: true
  # The configuration of the pipeline see https://opensearch.org/docs/2.4/data-prepper/pipelines/pipelines/
  config:
    otel-trace-pipeline:
      # workers is the number of threads processing data in each pipeline. 
      # We recommend same value for all pipelines.
      # default value is 1, set a value based on the machine you are running Data Prepper
      workers: 8 
      # delay in milliseconds is how often the worker threads should process data.
      # Recommend not to change this config as we want the entry-pipeline to process as quick as possible
      # default value is 3_000 ms
      delay: "100" 
      source:
        otel_trace_source:
          #record_type: event  # Add this when using Data Prepper 1.x. This option is removed in 2.0
          ssl: false # Change this to enable encryption in transit
          authentication:
            unauthenticated:
      buffer:
        bounded_blocking:
          # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. 
          # We recommend to keep the same buffer_size for all pipelines. 
          # Make sure you configure sufficient heap
          # default value is 512
          buffer_size: 512
          # This is the maximum number of request each worker thread will process within the delay.
          # Default is 8.
          # Make sure buffer_size >= workers * batch_size
          batch_size: 8
      processor:
        - trace_peer_forwarder:
        - aggregate:
            identification_keys: ["resource.attributes.componentId", "traceGroup", "traceId"]
            group_duration: "60s"
            action: 
              rate_limiter:
                events_per_second: 10  # Maximum 1000 events per minute per componentId
                when_exceeds: "drop"  # Drop events when rate exceeds 1000 per minute
      sink:
        - pipeline:
            name: "raw-trace-pipeline"
        - pipeline:
            name: "service-map-pipeline"
    raw-trace-pipeline:
      # Configure same as the otel-trace-pipeline
      workers: 8 
      # We recommend using the default value for the raw-pipeline.
      delay: "3000" 
      source:
        pipeline:
          name: "otel-trace-pipeline"
      buffer:
          bounded_blocking:
            # Configure the same value as in entry-pipeline
            # Make sure you configure sufficient heap
            # The default value is 512
            buffer_size: 512
            # The raw processor does bulk request to your OpenSearch sink, so configure the batch_size higher.
            # If you use the recommended otel-collector setup each ExportTraceRequest could contain max 50 spans. https://github.com/opensearch-project/data-prepper/tree/v0.7.x/deployment/aws
            # With 64 as batch size each worker thread could process upto 3200 spans (64 * 50)
            batch_size: 64
      processor:
        - aggregate:
            identification_keys: ["resource.attributes.componentId", "traceGroup", "traceId"]
            action:
              tail_sampler:
                percent: 50
                wait_period: "10s"
                condition: "/traceGroupFields/statusCode == 2"
            group_duration: "30s"
        - otel_trace_raw:
        - otel_trace_group:
            hosts: [ "https://opensearch-cluster-master:9200" ]
            # Change to your credentials
            username: "admin"
            password: "TraceDemo@123"
            insecure: true
            # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate  
            #cert: /path/to/cert
            # If you are connecting to an Amazon OpenSearch Service domain without
            # Fine-Grained Access Control, enable these settings. Comment out the
            # username and password above.
            #aws_sigv4: true
            #aws_region: us-east-1
      sink:
        - opensearch:
            hosts: [ "https://opensearch-cluster-master:9200" ]
            index_type: trace-analytics-raw
            # Change to your credentials
            username: "admin"
            password: "TraceDemo@123"
            insecure: true
            # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate  
            #cert: /path/to/cert
            # If you are connecting to an Amazon OpenSearch Service domain without
            # Fine-Grained Access Control, enable these settings. Comment out the
            # username and password above.
            #aws_sigv4: true
            #aws_region: us-east-1
    service-map-pipeline:
      workers: 8
      delay: "100"
      source:
        pipeline:
          name: "otel-trace-pipeline"
      processor:
        - service_map_stateful:
            # The window duration is the maximum length of time the data prepper stores the most recent trace data to evaluvate service-map relationships. 
            # The default is 3 minutes, this means we can detect relationships between services from spans reported in last 3 minutes.
            # Set higher value if your applications have higher latency. 
            window_duration: 180 
            insecure: true
      buffer:
          bounded_blocking:
            # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. 
            # We recommend to keep the same buffer_size for all pipelines. 
            # Make sure you configure sufficient heap
            # default value is 512
            buffer_size: 512
            # This is the maximum number of request each worker thread will process within the delay.
            # Default is 8.
            # Make sure buffer_size >= workers * batch_size
            batch_size: 8
      sink:
        - opensearch:
            hosts: [ "https://opensearch-cluster-master:9200" ]
            index_type: trace-analytics-service-map
            # Change to your credentials
            username: "admin"
            password: "TraceDemo@123"
            insecure: true
      

# -- Data Prepper ports
ports:
  # -- The port that the source is running on. Default value is 2021. Valid options are between 0 and 65535.
  # https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/sources/http-source/
  - name: http-source
    port: 2021
  # -- The port that the otel_trace_source source runs on. Default value is 21890.
  # https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/sources/otel-trace-source/
  - name: otel-traces
    port: 21890
  # -- The port that the OpenTelemtry metrics source runs on. Default value is 21891.
  # https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/sources/otel-metrics-source/
  - name: otel-metrics
    port: 21891
  # -- Represents the port that the otel_logs_source source is running on. Default value is 21892.
  # https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/sources/otel-logs-source/
  - name: otel-logs
    port: 21892
  - name: api
    port: 4900

serviceAccount:
  # -- Specifies whether a service account should be created
  create: true
  # -- Automatically mount a ServiceAccount's API credentials?
  automount: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

initContainers: []

service:
  type: ClusterIP

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}
